* Data
** Hyperlinks

Cedars-Sinai dataset. See [[http://www.ncbi.nlm.nih.gov/pubmed/26362074][Machine learning approaches to analyze
histological images of tissues from radical prostatectomies]] for more
details about the curation, annotation and so on.

** Description

There are 225 images. In this analysis, they are tiled with a stride
of tile length / 2. The class is selected from the center pixel
(rounded down if length is even). Experiments are run with patch sizes
of 32, 64, 128.

AT masks are used. There are 4 pixel-wise classes in this data.

The AT masks correspond to pathologist labelling: 0,1,2,3 corresponds
to yellow, red, blue, green (see color annotated images) which
corresponds to stroma, high grade tumor, benign/normal, low grade
tumor.

| 1 | Y | stroma        |
| 2 | R | high grade    |
| 3 | B | benign/normal |
| 4 | G | low grade     |

For a patch size of 64, after train/test/validation split, we are left
with $\sim 200,000$ training patches and $\sim 50,000$ validation patches. Test set
has no been touched yet.

#+ATTR_LATEX: :width 1.20\textwidth :placement {l}{-1.0\textwidth}
[[./all-grades.jpg]]

** Stats
   
After separating out hard hold out set, here are some stats:

patch size = 64
only looking at train set (not validation set)
number of patches of each class:
[26286, 51601, 27940, 91741]

So the classes are very unbalanced. As of now, no class balancing is
done.

* Model
  
ResNet model with bottleneck blocks. All convolutions (and max pooling) use /same padding/. Assume an input patch of size 64 x 64 x 3.

1. (7 x 7) x 64 channel convolution. Output is (64 x 64) x 64.

2. (3 x 3) max pool with stride (2 x 2). Output is (32 x 32) x 64.

3. The first group has 3 bottleneck blocks. Each one is defined by an input/output filter size (the first one is 128 filter) and a botteneck size (the first one is 32 filters).
   
4. First the input to the group is scaled using one by one convolutions to have the correct number of filters. In this case, (1 x 1) x 128 convolution to get to the input/output size of 128.
  
5. This is followed by the three bottleneck blocks of the same dimensions:
    - (1 x 1) x 32 convolution. Output is (32 x 32) x 32.
    - (3 x 3) x 32 convolution. Output is (32 x 32) x 32.
    - (1 x 1) x 64 (num of input channels to the block). Output is (32 x 32) x 64.
    - Skip connection: add the input from step () to the output of the previous step.

# There are 4 bottleneck groups, each one has 3 bottleneck blocks, each
# block has 3 layers --- an expansion layer which does 1x1 convolution
# with a filter size of 128, 256, 512, and 1024 for each group, a
# bottleneck layer which does 3x3 convolution with the same number of
# filters and a 3x3 convolution with the number of filters set to the
# input of the bottleneck block so that dimensions match for the
# residual connection.

# In total there are $4 \times 3 \times 3 = 36$ layers.

* Results
  
Honestly these results looks terrible. First of all, there should be
75% test error from a random model. Secondly, the model converges
immediately. Why?

*** Half of the data

#+ATTR_LATEX: :width 9cm
[[./cedars_sinai_5dataset_trainerr.png]]

#+ATTR_LATEX: :width 9cm
[[./cedars_sinai_5dataset_testerr.png]]

*** All of the data

#+ATTR_LATEX: :width 9cm
[[./cedars_sinai_fulldataset_trainerr.png]]

#+ATTR_LATEX: :width 9cm
[[./cedars_sinai_fulldataset_testerr.png]]

#+CAPTION: confusion matrix for patch size 64, model trained on all the training data. Calculated from test data. Rows are the true class and columns are the predicted class. (Wall time: $\sim 16\text{ min}$.)
| 3243 | 1763 |  300 |  1140 |
|  602 | 9952 |  192 |  1983 |
|  462 |  354 | 5346 |   902 |
|  481 | 1394 |  495 | 20783 |

#+CAPTION: final training and test errors after 14 epochs on the full dataset.
| last batch training error                                 | 0.34 |
| last epoch test error (averaged over entire training set) | 0.22 |


# #+BEGIN_SRC python
# with open("/mnt/code/notebooks/results/frac_data=all_correctmodel.json") as f:
#     results = json.load(f)
#     preds = pickle.loads(results['test_predictions'])
#     ytest = np.load("/mnt/data/output/ytest.npy")
#     sklearn.metrics.confusion_matrix(ytest, np.argmax(preds, axis=1))
# #+END_SRC

# test1  - labels 1, 4
# test4  - labels 1, 2, 3, 4
# test33 - labels 3, 4

* Research and Benchmarks

# What is the meaning of a 4 class mask as in the AT Mask? What is the
# different between AT masks and the ST GL masks? How does this
# correspond to the Cedars-Sinai paper?

Should I use Jaccard index as well even though it seems wrong and
somewhat convoluted? This is what the creators of the dataset
use. Perhaps I just make sure the equations work out and then
=s/Jaccard/accuracy/= or something like that.

* Next Steps

- confusion matrix
- overlay of predictions --> stride of 1
- segmentation by classification
- fully convolutional neural networks
- predict percentage?
- predict grade? predict high-low?
- other datasets (MSK data)
- need to talk to a pathologist and see what's truly important / useful
