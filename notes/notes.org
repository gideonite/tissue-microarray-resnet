* Data
** Hyperlinks

Cedars-Sinai dataset. See [[http://www.ncbi.nlm.nih.gov/pubmed/26362074][Machine learning approaches to analyze
histological images of tissues from radical prostatectomies]] for more
details about the curation, annotation and so on.

** Description

There are 225 images. In this analysis, they are tiled with a stride
of tile length / 2. The class is selected from the center pixel
(rounded down if length is even). Experiments are run with patch sizes
of 32, 64, 128.

AT masks are used. There are 4 pixel-wise classes in this data.

The AT masks correspond to pathologist labelling: 0,1,2,3 corresponds
to yellow, red, blue, green (see color annotated images) which
corresponds to stroma, high grade tumor, benign/normal, low grade
tumor.

| 1 | Y | stroma        |
| 2 | R | high grade    |
| 3 | B | benign/normal |
| 4 | G | low grade     |

#+ATTR_LATEX: :width 1.00\textwidth :placement {l}{-1.0\textwidth}
[[./all-grades.jpg]]

** Stats
   
After separating out hard hold out set, here are some stats:

patch size = 64
only looking at train set (not validation set)
number of patches of each class:
[26286, 51601, 27940, 91741]

So the classes are very unbalanced. As of now, no class balancing is
done.

* Model
  
ResNet model with bottleneck blocks. All convolutions (and max pooling) use /same padding/. Assume an input patch of size 64 x 64 x 3.

(7 x 7) x 64 channel convolution. Output is (64 x 64) x 64.

(3 x 3) max pool with stride (2 x 2). Output is (32 x 32) x 64.

The first group has 3 blocks, 128 filters (for each block) and 32
bottleneck size (for each block).

(1 x 1) x 32 convolution. Output is (32 x 32) x 32.

(3 x 3) x 32 convolution. Output is (32 x 32) x 32.

(1 x 1) x 64 (num of input channels to the block). Output is (32 x 32) x 64.

Skip connection: add the input from step () to the output of the previous step.



# There are 4 bottleneck groups, each one has 3 bottleneck blocks, each
# block has 3 layers --- an expansion layer which does 1x1 convolution
# with a filter size of 128, 256, 512, and 1024 for each group, a
# bottleneck layer which does 3x3 convolution with the same number of
# filters and a 3x3 convolution with the number of filters set to the
# input of the bottleneck block so that dimensions match for the
# residual connection.

# In total there are $4 \times 3 \times 3 = 36$ layers.

* Results

#+CAPTION: confusion matrix for patch size 64, model trained on all the training data. Calculated from test data. Rows are the true class and columns are the predicted class. Wall time: ~16min.
| 4132 |  1002 |  444 |   868 |
|  862 | 10016 |  254 |  1597 |
|  170 |   132 | 6136 |   626 |
|  817 |  1225 |  596 | 20515 |

test1  - labels 1, 4
test4  - labels 1, 2, 3, 4
test33 - labels 3, 4


* Research and Benchmarks

# What is the meaning of a 4 class mask as in the AT Mask? What is the
# different between AT masks and the ST GL masks? How does this
# correspond to the Cedars-Sinai paper?

Should I use Jaccard index as well even though it seems wrong and
somewhat convoluted?

* Next Steps

- confusion matrix
- overlay of predictions --> stride of 1
- segmentation by classification
- fully convolutional neural networks
- predict percentage?
- predict grade? predict high-low?
- other datasets (MSK data)
- eventually need to talk to a pathologist
